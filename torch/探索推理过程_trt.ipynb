{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 探索下 trt 的代码\n",
    "\n",
    "主要是 transformer-deploy_github/src/transformer_deploy/backends/trt_utils.py 下的.\n",
    "主要还是熟悉下 API 和使用流程.\n",
    "\n",
    "[TensorRT Core Concepts](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/coreConcepts.html)\n",
    "\n",
    "The general TensorRT workflow consists of 3 steps:\n",
    "\n",
    "1. Populate a tensorrt.INetworkDefinition either with a parser or by using the TensorRT Network API (see tensorrt.INetworkDefinition for more details). The tensorrt.Builder can be used to generate an empty tensorrt.INetworkDefinition .\n",
    "2. Use the tensorrt.Builder to build a tensorrt.ICudaEngine using the populated tensorrt.INetworkDefinition .\n",
    "3. Create a tensorrt.IExecutionContext from the tensorrt.ICudaEngine and use it to perform optimized inference.\n",
    "\n",
    "\n",
    "补充一下 CUDA 的基础概念 [CUDA SEMANTICS](https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from tensorrt import ICudaEngine, IExecutionContext, ILayer, INetworkDefinition, Logger, Runtime\n",
    "from tensorrt.tensorrt import Builder, IBuilderConfig, IElementWiseLayer, IOptimizationProfile, IReduceLayer, OnnxParser\n",
    "\n",
    "import logging\n",
    "from typing import Callable, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binding_idxs(engine: trt.ICudaEngine, profile_index: int):\n",
    "    \"\"\"\n",
    "    Calculate start/end binding indices for current context's profile\n",
    "    https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles_bindings\n",
    "    :param engine: TensorRT engine generated during the model building\n",
    "    :param profile_index: profile to use (several profiles can be set during building)\n",
    "    :return: input and output tensor indexes\n",
    "    \"\"\"\n",
    "    # num_bindings 就是输入和输出的名字数量. num_optimization_profiles 是优化配置文件的数量\n",
    "    num_bindings_per_profile = engine.num_bindings // engine.num_optimization_profiles\n",
    "    # 开始的索引位置\n",
    "    start_binding = profile_index * num_bindings_per_profile\n",
    "    # 结束的索引位置\n",
    "    end_binding = start_binding + num_bindings_per_profile  # Separate input and output binding indices for convenience\n",
    "    input_binding_idxs: List[int] = []\n",
    "    output_binding_idxs: List[int] = []\n",
    "    # 判断每个索引位置是否是输入, 将输入和输出的索引分别装到数组中\n",
    "    for binding_index in range(start_binding, end_binding):\n",
    "        if engine.binding_is_input(binding_index):\n",
    "            input_binding_idxs.append(binding_index)\n",
    "        else:\n",
    "            output_binding_idxs.append(binding_index)\n",
    "    return input_binding_idxs, output_binding_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_file_path=\"./onnx/model_torch.trt\"\n",
    "\n",
    "# Logger for the Builder, ICudaEngine and Runtime . 日志服务类\n",
    "trt_logger: Logger = trt.Logger(trt.Logger.ERROR)\n",
    "# Allows a serialized ICudaEngine to be deserialized. 运行时, 用来承载模型\n",
    "runtime: Runtime = trt.Runtime(trt_logger)\n",
    "profile_index = 0\n",
    "\n",
    "with open(file=engine_file_path, mode=\"rb\") as f:\n",
    "    # 反序列化, 返回 ICudaEngine. An ICudaEngine for executing inference on a built network.\n",
    "    engine: ICudaEngine = runtime.deserialize_cuda_engine(f.read())\n",
    "    # A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams. See CUDA semantics for details.\n",
    "    # .cuda_stream 属性好像不在 torch 的文档上. 而且类型是 int\n",
    "    stream: int = torch.cuda.current_stream().cuda_stream\n",
    "    # Create an IExecutionContext . 创建一个执行上下文\n",
    "    # Context for executing inference using an ICudaEngine . Multiple IExecutionContext s may exist for one ICudaEngine instance, allowing the same ICudaEngine to be used for the execution of multiple batches simultaneously.\n",
    "    context: IExecutionContext = engine.create_execution_context()\n",
    "    # Set the optimization profile with async semantics. 加载优化配置文件\n",
    "    context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream)\n",
    "    # retrieve input/output IDs. 获取输入和输出的索引\n",
    "    input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index)  # type: List[int], List[int]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 目前看到的好多方法, 都是要求这个值为 False 的\n",
    "engine.has_implicit_batch_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import torch\n",
    "import numpy\n",
    "from transformers import BertTokenizer\n",
    "enc = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "masked_sentences = ['Paris is the [MASK] of France.', \n",
    "                    'The primary [MASK] of the United States is English.', \n",
    "                    'A baseball game consists of at least nine [MASK].', \n",
    "                    'Topology is a branch of [MASK] concerned with the properties of geometric objects that remain unchanged under continuous transformations.']\n",
    "pos_masks = [4, 3, 9, 6]\n",
    "\n",
    "inputs = enc(masked_sentences, return_tensors=\"np\", padding='max_length', max_length=128, truncation=True)\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = enc(masked_sentences[:1], return_tensors=\"pt\", padding='max_length', max_length=128, truncation=True)\n",
    "inputs[\"input_ids\"].device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_tensors(\n",
    "    context: trt.IExecutionContext,\n",
    "    host_inputs: List[torch.Tensor],\n",
    "    input_binding_idxs: List[int],\n",
    "    output_binding_idxs: List[int],\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    保留内存.\n",
    "    Reserve memory in GPU for input and output tensors.\n",
    "    :param context: TensorRT context shared accross inference steps\n",
    "    :param host_inputs: input tensor\n",
    "    :param input_binding_idxs: indexes of each input vector (should be the same than during building)\n",
    "    :param output_binding_idxs: indexes of each output vector (should be the same than during building)\n",
    "    :return: tensors where output will be stored\n",
    "    \"\"\"\n",
    "    # explicitly set dynamic input shapes, so dynamic output shapes can be computed internally\n",
    "    for host_input, binding_index in zip(host_inputs, input_binding_idxs):\n",
    "        # Set the dynamic shape of a binding. 设置动态形状, 根据这个输入的形状\n",
    "        context.set_binding_shape(binding_index, tuple(host_input.shape))\n",
    "    # assert context.all_binding_shapes_specified\n",
    "    device_outputs: Dict[str, torch.Tensor] = dict()\n",
    "    for binding_index in output_binding_idxs:\n",
    "        # 获取输出的形状\n",
    "        # TensorRT computes output shape based on input shape provided above\n",
    "        output_shape = context.get_binding_shape(binding=binding_index)\n",
    "        # 输出的名字\n",
    "        output_name = context.engine.get_binding_name(index=binding_index)\n",
    "        # 分配 GPU 内存空间\n",
    "        # allocate buffers to hold output results\n",
    "        device_outputs[output_name] = torch.empty(tuple(output_shape), device=\"cuda\")\n",
    "    return device_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tensorrt(\n",
    "    context: IExecutionContext,\n",
    "    inputs: Dict[str, torch.Tensor],\n",
    "    input_binding_idxs: List[int],\n",
    "    output_binding_idxs: List[int],\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    执行推理\n",
    "    Perform inference with TensorRT.\n",
    "    :param context: shared variable\n",
    "    :param inputs: input tensor\n",
    "    :param input_binding_idxs: input tensor indexes\n",
    "    :param output_binding_idxs: output tensor indexes\n",
    "    :return: output Dict[tensor name, tensor value]\n",
    "    \"\"\"\n",
    "\n",
    "    input_tensors: List[torch.Tensor] = list()\n",
    "    # 对于每个名字的索引位置. 现在是按名字读取的输入, 所以 dict 的顺序就不重要了\n",
    "    for i in range(context.engine.num_bindings):\n",
    "        # 判断是否是输入位置\n",
    "        if not context.engine.binding_is_input(index=i):\n",
    "            continue\n",
    "        # 输入的名字\n",
    "        tensor_name = context.engine.get_binding_name(i)\n",
    "        assert tensor_name in inputs, f\"input not provided: {tensor_name}\"\n",
    "        tensor = inputs[tensor_name]\n",
    "        assert isinstance(tensor, torch.Tensor), f\"unexpected tensor class: {type(tensor)}\"\n",
    "        # 需要在 cuda 上. 原来如此, 旧的 v0.4.0 版本会手动给你复制到 cuda 上, 所以不会有这个报错\n",
    "        assert tensor.device.type == \"cuda\", f\"unexpected device type (trt only works on CUDA): {tensor.device.type}\"\n",
    "        # 类型会被截断到 int32 上\n",
    "        # warning: small changes in output if int64 is used instead of int32\n",
    "        if tensor.dtype in [torch.int64, torch.long]:\n",
    "            logging.warning(f\"using {tensor.dtype} instead of int32 for {tensor_name}, will be casted to int32\")\n",
    "            tensor = tensor.type(torch.int32)\n",
    "        input_tensors.append(tensor)\n",
    "    # 继续深入, 实际上还是要看这个函数\n",
    "    # calculate input shape, bind it, allocate GPU memory for the output\n",
    "    outputs: Dict[str, torch.Tensor] = get_output_tensors(\n",
    "        context, input_tensors, input_binding_idxs, output_binding_idxs\n",
    "    )\n",
    "    # data_prt 返回第一个元素的地址 Returns the address of the first element of self tensor.\n",
    "    bindings = [int(i.data_ptr()) for i in input_tensors + list(outputs.values())]\n",
    "    # Asynchronously execute inference on a batch. 这个是异步执行的, 所以下面需要使用 synchronize\n",
    "    assert context.execute_async_v2(\n",
    "        bindings, torch.cuda.current_stream().cuda_stream\n",
    "    ), \"failure during execution of inference\"\n",
    "    # 等待完成, 相当于强制同步\n",
    "    torch.cuda.current_stream().synchronize()  # sync all CUDA ops\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2] [3]\n",
      "(1, 128)\n",
      "(1, 128)\n",
      "(1, 128)\n",
      "(1, 128, 30522)\n"
     ]
    }
   ],
   "source": [
    "# 这些就是输入和输出的维度, 这个trt模型没有动态维度\n",
    "print(input_binding_idxs, output_binding_idxs)\n",
    "for x in input_binding_idxs + output_binding_idxs:\n",
    "    print(context.get_binding_shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:using torch.int64 instead of int32 for input_ids, will be casted to int32\n",
      "WARNING:root:using torch.int64 instead of int32 for attention_mask, will be casted to int32\n",
      "WARNING:root:using torch.int64 instead of int32 for token_type_ids, will be casted to int32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[[-6.5416, -6.5076, -6.5212,  ..., -5.8961, -5.7352, -3.8943],\n",
       "          [-9.0158, -9.0488, -9.0614,  ..., -8.2589, -8.0348, -6.1794],\n",
       "          [-8.6559, -9.0902, -8.7755,  ..., -7.4504, -5.3415, -9.6577],\n",
       "          ...,\n",
       "          [-8.8079, -9.0085, -8.9305,  ..., -8.1445, -9.2694, -5.3116],\n",
       "          [-8.7577, -8.8732, -8.8879,  ..., -8.3378, -9.3688, -4.9300],\n",
       "          [-8.7828, -9.0249, -8.9177,  ..., -8.3143, -8.8779, -6.5312]]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以了, 应该要尝试进行推理了\n",
    "inputs = enc(masked_sentences[:1], return_tensors=\"pt\", padding='max_length', max_length=128, truncation=True)\n",
    "inputs = dict((k, v.to(\"cuda\")) for k, v in inputs.items())\n",
    "infer_tensorrt(context, inputs, input_binding_idxs, output_binding_idxs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
