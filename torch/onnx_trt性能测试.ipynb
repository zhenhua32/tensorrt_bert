{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime-gpu==1.11 -i http://pypi.douban.com/simple --trusted-host pypi.douban.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install netron -i http://pypi.douban.com/simple --trusted-host pypi.douban.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -i http://pypi.douban.com/simple --trusted-host pypi.douban.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/examples\n"
     ]
    }
   ],
   "source": [
    "# 我人都傻了, 我说怎么找不到文件, 前面该 notebook 放在 /workspace 目录下, pwd 居然是根目录 /. 真魔幻\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "GPU\n",
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "print(onnxruntime.__version__)\n",
    "print(onnxruntime.get_device())\n",
    "print(onnxruntime.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import torch\n",
    "import numpy\n",
    "from transformers import BertTokenizer\n",
    "enc = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "masked_sentences = ['Paris is the [MASK] of France.', \n",
    "                    'The primary [MASK] of the United States is English.', \n",
    "                    'A baseball game consists of at least nine [MASK].', \n",
    "                    'Topology is a branch of [MASK] concerned with the properties of geometric objects that remain unchanged under continuous transformations.']\n",
    "pos_masks = [4, 3, 9, 6]\n",
    "\n",
    "inputs = enc(masked_sentences, return_tensors=\"np\", padding='max_length', max_length=128, truncation=True)\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "origin_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", torchscript=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地转换模型还是有点报错的, 输出里提到绝对误差超过了 1e-5, 不给导出\n",
    "!python -m transformers.onnx --model=bert-base-uncased --feature=masked-lm onnx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids int64 (4, 128)\n",
      "token_type_ids int64 (4, 128)\n",
      "attention_mask int64 (4, 128)\n"
     ]
    }
   ],
   "source": [
    "for key, val in inputs.items():\n",
    "    print(key, val.dtype, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-5.3025, -5.4727, -5.5078,  ..., -5.2318, -7.0131,  0.6104],\n",
       "          [-4.8874, -5.1045, -5.1398,  ..., -4.6891, -6.6637,  0.6894],\n",
       "          [-4.4334, -4.6704, -4.7000,  ..., -4.4374, -6.2480,  1.1334],\n",
       "          ...,\n",
       "          [-4.2189, -4.4351, -4.4214,  ..., -4.1686, -5.9111,  1.2778],\n",
       "          [-4.0962, -4.3313, -4.3119,  ..., -4.0072, -5.8356,  1.4033],\n",
       "          [-4.0386, -4.2785, -4.2459,  ..., -3.8926, -5.7575,  1.3704]]],\n",
       "        grad_fn=<ViewBackward0>),)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(0, 100, (4, 128), device=\"cpu\", dtype=torch.int64)\n",
    "attention_mask = torch.randint(0, 2, (4, 128), device=\"cpu\", dtype=torch.int64)\n",
    "token_type_ids = torch.randint(0, 2, (4, 128), device=\"cpu\", dtype=torch.int64)\n",
    "\n",
    "traced_origin_model = torch.jit.trace(origin_model, [input_ids, attention_mask, token_type_ids])\n",
    "traced_origin_model(input_ids, attention_mask, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.douban.com/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch==1.13\n",
      "  Downloading http://pypi.doubanio.com/packages/06/3d/19b2090f3f01751e816d409edcc808c82c2973348713186ac153e735d49f/torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl (890.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 890.2 MB 1.6 MB/s eta 0:00:018     |██████████                      | 277.8 MB 379 kB/s eta 0:26:53     |█████████████▉                  | 384.2 MB 388 kB/s eta 0:21:42     |█████████████████▎              | 479.1 MB 290 kB/s eta 0:23:36     |███████████████████████▍        | 651.0 MB 304 kB/s eta 0:13:06\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.13) (4.2.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.8/site-packages (from torch==1.13) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.8/site-packages (from torch==1.13) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch==1.13) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch==1.13) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13) (59.5.0)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0\n",
      "    Uninstalling torch-1.11.0:\n",
      "      Successfully uninstalled torch-1.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0a0 requires torch==1.12.0a0+8a1a93a, but you have torch 1.13.0 which is incompatible.\n",
      "torchtext 0.13.0a0 requires torch==1.12.0a0+8a1a93a, but you have torch 1.13.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.13 -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.douban.com/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: onnx in /opt/conda/lib/python3.8/site-packages (1.11.0)\n",
      "Collecting onnx\n",
      "  Downloading http://pypi.doubanio.com/packages/71/77/8ee74cd1a4f5f776794a9cff11d7fa8375189e60cd81937744f81bd92e72/onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.5 MB 534 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.8/site-packages (from onnx) (4.2.0)\n",
      "Collecting protobuf<4,>=3.20.2\n",
      "  Downloading http://pypi.doubanio.com/packages/da/e4/4d62585593e9f962cb02614534f62f930de6a80a0a3784282094a01919b2/protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.8/site-packages (from onnx) (1.22.3)\n",
      "Installing collected packages: protobuf, onnx\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: onnx\n",
      "    Found existing installation: onnx 1.11.0\n",
      "    Uninstalling onnx-1.11.0:\n",
      "      Successfully uninstalled onnx-1.11.0\n",
      "Successfully installed onnx-1.13.0 protobuf-3.20.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U onnx -i http://pypi.douban.com/simple --trusted-host pypi.douban.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `torch.onnx.export` 导出模型的时候需要注意时间, 一般 5 分钟内没导出就需要检查了.\n",
    "\n",
    "目前尝试的方式是\n",
    "1. 升级 onnx 版本\n",
    "2. 升级或降级 torch 版本 (当前降级到 1.11 版本可以导出模型)\n",
    "3. 还有个最大的原因, 可能是 notebook 造成的, 直接写在 py 代码里运行是可以的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(origin_model.config.use_cache)\n",
    "setattr(origin_model.config, \"use_cache\", False)\n",
    "print(origin_model.config.use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出 ONNX 模型, 宇宙级深坑, 注意检查 jupyter, 直接用 python 代码可以运行的\n",
    "# 具体和 /opt/conda/lib/python3.8/site-packages/transformers/onnx/convert.py 的 export_pytorch 的函数也没看出有多大的不同\n",
    "\n",
    "symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "with torch.no_grad():\n",
    "    traced_origin_model.eval()\n",
    "    torch.onnx.export(\n",
    "        traced_origin_model,\n",
    "        args=(input_ids, attention_mask, token_type_ids),\n",
    "        f=\"model_torch.onnx\",\n",
    "        export_params=True,\n",
    "        verbose=True,\n",
    "        input_names=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "        output_names=[\"logits\"],\n",
    "        opset_version=16,\n",
    "        do_constant_folding=True,\n",
    "        # 设置动态 shape\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": symbolic_names,\n",
    "            \"attention_mask\" : symbolic_names,\n",
    "            \"token_type_ids\" : symbolic_names,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.checker.check_model(onnx.load(\"./onnx/model_torch.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2023-12:35:23] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:368: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[01/11/2023-12:35:25] [TRT] [W] Output type must be INT32 for shape outputs\n",
      "[01/11/2023-12:35:25] [TRT] [W] Output type must be INT32 for shape outputs\n",
      "[01/11/2023-12:35:25] [TRT] [W] Output type must be INT32 for shape outputs\n",
      "[01/11/2023-12:35:25] [TRT] [W] Output type must be INT32 for shape outputs\n",
      "[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored\n",
      "[I]     Configuring with profiles: [Profile().add(input_ids, min=[1, 128], opt=[1, 128], max=[1, 128]).add(attention_mask, min=[1, 128], opt=[1, 128], max=[1, 128]).add(token_type_ids, min=[1, 128], opt=[1, 128], max=[1, 128])]\n",
      "[I] Building engine with configuration:\n",
      "    Workspace            | 16777216 bytes (16.00 MiB)\n",
      "    Precision            | TF32: False, FP16: False, INT8: False, Strict Types: False\n",
      "    Tactic Sources       | ['CUBLAS', 'CUBLAS_LT', 'CUDNN']\n",
      "    Safety Restricted    | False\n",
      "    Profiles             | 1 profile(s)\n",
      "[I] Finished engine building in 13.978 seconds\n",
      "[I] Saving engine to ./onnx/model_torch.trt\n"
     ]
    }
   ],
   "source": [
    "# 将 onnx 导出成 tensort 格式\n",
    "!polygraphy convert ./onnx/model_torch.onnx --convert-to trt -o ./onnx/model_torch.trt --model-type onnx \\\n",
    "    --trt-min-shapes input_ids:[1,128] attention_mask:[1,128] token_type_ids:[1,128] \\\n",
    "    --trt-opt-shapes input_ids:[1,128] attention_mask:[1,128] token_type_ids:[1,128] \\\n",
    "    --trt-max-shapes input_ids:[1,128] attention_mask:[1,128] token_type_ids:[1,128]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义三种格式的预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchModel:\n",
    "    def __init__(self):\n",
    "        self.enc = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", torchscript=True).eval().cuda()\n",
    "    \n",
    "    def predict(self, texts: list, pos_masks: list, return_numpy=True):\n",
    "        \"\"\"\n",
    "        texts 是数组\n",
    "        pos_masks 是数组,  [MASK] 掩码所在的位置, 注意, 位置需要 +1, 因为前面会填充一个 [CLS] 字符\n",
    "        'Paris is the [MASK] of France.' 对应的 pos_mask 是 4\n",
    "        \"\"\"\n",
    "        inputs = self.enc(texts, return_tensors=\"pt\", padding='max_length', max_length=128, truncation=True)\n",
    "        inputs = dict((k, v.cuda()) for k, v in inputs.items())\n",
    "        with torch.no_grad():\n",
    "            result = self.model(**inputs)\n",
    "        if return_numpy:\n",
    "            result = [x.cpu().numpy() for x in result]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[[-6.541635 , -6.5075865, -6.5212164, ..., -5.8960814,\n",
       "          -5.735184 , -3.8943412],\n",
       "         [-9.015771 , -9.048779 , -9.0614195, ..., -8.258874 ,\n",
       "          -8.034796 , -6.17945  ],\n",
       "         [-8.655979 , -9.090192 , -8.775549 , ..., -7.450444 ,\n",
       "          -5.341511 , -9.657742 ],\n",
       "         ...,\n",
       "         [-8.807926 , -9.008511 , -8.930504 , ..., -8.144446 ,\n",
       "          -9.269405 , -5.3116097],\n",
       "         [-8.757712 , -8.873206 , -8.887868 , ..., -8.337829 ,\n",
       "          -9.368811 , -4.929955 ],\n",
       "         [-8.782769 , -9.024868 , -8.917733 , ..., -8.314313 ,\n",
       "          -8.877869 , -6.53116  ]]], dtype=float32)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model = TorchModel()\n",
    "torch_model.predict(masked_sentences[:1], pos_masks[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ONNXModel:\n",
    "    def __init__(self) -> None:\n",
    "        self.enc = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = onnxruntime.InferenceSession(\"./onnx/model_torch.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "    \n",
    "    def predict(self, texts: list, pos_masks: list):\n",
    "        # 输入的数据类型是 np\n",
    "        inputs = self.enc(texts, return_tensors=\"np\", padding='max_length', max_length=128, truncation=True)\n",
    "        result = self.model.run(None, input_feed=dict(inputs))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[-6.541909 , -6.5074854, -6.5217304, ..., -5.8963327,\n",
       "          -5.735438 , -3.8944366],\n",
       "         [-9.016863 , -9.049933 , -9.063157 , ..., -8.260457 ,\n",
       "          -8.036411 , -6.180646 ],\n",
       "         [-8.652298 , -9.086587 , -8.771991 , ..., -7.447172 ,\n",
       "          -5.337555 , -9.656389 ],\n",
       "         ...,\n",
       "         [-8.808946 , -9.009913 , -8.932179 , ..., -8.146702 ,\n",
       "          -9.2728195, -5.312007 ],\n",
       "         [-8.759872 , -8.875669 , -8.890579 , ..., -8.340458 ,\n",
       "          -9.3715725, -4.933445 ],\n",
       "         [-8.783872 , -9.026353 , -8.919335 , ..., -8.316137 ,\n",
       "          -8.880703 , -6.534636 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model = ONNXModel()\n",
    "onnx_model.predict(masked_sentences[:1], pos_masks[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformer-deploy'...\n",
      "remote: Enumerating objects: 2099, done.\u001b[K\n",
      "remote: Counting objects: 100% (441/441), done.\u001b[K\n",
      "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
      "remote: Total 2099 (delta 307), reused 330 (delta 247), pack-reused 1658\u001b[K\n",
      "Receiving objects: 100% (2099/2099), 33.96 MiB | 82.00 KiB/s, done.\n",
      "Resolving deltas: 100% (1099/1099), done.\n",
      "Note: switching to 'ccfeb215fda7ee635068f239b345ab7380f61c6b'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone -b v0.4.0 https://github.com/ELS-RD/transformer-deploy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from tensorrt import Logger, Runtime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"./transformer-deploy/src/\")\n",
    "\n",
    "from transformer_deploy.backends.trt_utils import build_engine, load_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ONNXTensorrtModel:\n",
    "    def __init__(self) -> None:\n",
    "        self.enc = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        trt_logger: Logger = trt.Logger(trt.Logger.ERROR)\n",
    "        runtime: Runtime = trt.Runtime(trt_logger)\n",
    "        profile_index = 0\n",
    "        self.model = load_engine(\n",
    "            runtime=runtime,\n",
    "            engine_file_path=\"./onnx/model_torch.trt\",\n",
    "            profile_index=profile_index,\n",
    "        )\n",
    "    \n",
    "    def predict(self, texts: list, pos_masks: list, return_numpy=True):\n",
    "        # 输入的数据类型是 pt\n",
    "        inputs = self.enc(texts, return_tensors=\"pt\", padding='max_length', max_length=128, truncation=True)\n",
    "        # 略坑, 需要调整顺序, 所以我不知道这里传入个 dict 有什么意义\n",
    "        new_inputs = dict()\n",
    "        for key in ['input_ids', 'attention_mask', 'token_type_ids']:\n",
    "            new_inputs[key] = inputs[key]\n",
    "        result = self.model(new_inputs)\n",
    "        if return_numpy:\n",
    "            result = [x.cpu().numpy() for x in result]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[-6.541634 , -6.507587 , -6.5212154, ..., -5.896081 ,\n",
       "          -5.735177 , -3.8943386],\n",
       "         [-9.015753 , -9.048758 , -9.0613985, ..., -8.25886  ,\n",
       "          -8.034779 , -6.179438 ],\n",
       "         [-8.655941 , -9.090158 , -8.775516 , ..., -7.4504123,\n",
       "          -5.341491 , -9.657685 ],\n",
       "         ...,\n",
       "         [-8.80792  , -9.008501 , -8.930499 , ..., -8.144452 ,\n",
       "          -9.269407 , -5.311605 ],\n",
       "         [-8.757726 , -8.873215 , -8.887872 , ..., -8.337824 ,\n",
       "          -9.368818 , -4.929951 ],\n",
       "         [-8.782761 , -9.024862 , -8.917737 , ..., -8.3143015,\n",
       "          -8.877874 , -6.531156 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt_model = ONNXTensorrtModel()\n",
    "trt_model.predict(masked_sentences[:1], pos_masks[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = torch_model.predict(masked_sentences[:1], pos_masks[:1])[0]\n",
    "b = onnx_model.predict(masked_sentences[:1], pos_masks[:1])[0]\n",
    "c = trt_model.predict(masked_sentences[:1], pos_masks[:1])[0]\n",
    "\n",
    "print(numpy.allclose(a, b, rtol=1e-03, atol=1e-3))\n",
    "print(numpy.allclose(a, c, rtol=1e-03, atol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3007] [18.199736]\n",
      "Paris is the capital of France.\n",
      "[3007] [18.201002]\n",
      "Paris is the capital of France.\n",
      "[3007] [18.199774]\n",
      "Paris is the capital of France.\n"
     ]
    }
   ],
   "source": [
    "def print_result(result, masked_sentences, pos_masks):\n",
    "    most_likely_token_ids = [numpy.argmax(result[i, pos, :]) for i, pos in enumerate(pos_masks)]\n",
    "    most_likely_probs = [numpy.max(result[i, pos, :]) for i, pos in enumerate(pos_masks)]\n",
    "    print(most_likely_token_ids, most_likely_probs)\n",
    "    unmasked_tokens = enc.decode(most_likely_token_ids).split(' ')\n",
    "    unmasked_sentences = [masked_sentences[i].replace('[MASK]', token) for i, token in enumerate(unmasked_tokens)]\n",
    "    for sentence in unmasked_sentences:\n",
    "        print(sentence)\n",
    "\n",
    "print_result(a, masked_sentences[:1], pos_masks[:1])\n",
    "print_result(b, masked_sentences[:1], pos_masks[:1])\n",
    "print_result(c, masked_sentences[:1], pos_masks[:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单测试下性能\n",
    "或许应该将输出归一化成一样的类型, 因为从 cuda 上的 tensor 转换成 numpy 也很耗时的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import timeit\n",
    "def timeGraph(call_func, num_loops=50):\n",
    "    print(\"Warm up ...\")\n",
    "    for _ in range(20):\n",
    "        call_func()\n",
    "\n",
    "    # 等待同步, cuda 默认是异步调用的\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    for i in range(num_loops):\n",
    "        start_time = timeit.default_timer()\n",
    "        call_func()\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = timeit.default_timer()\n",
    "        timings.append(end_time - start_time)\n",
    "        # print(\"Iteration {}: {:.6f} s\".format(i, end_time - start_time))\n",
    "\n",
    "    return timings\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def printStats(graphName, timings, batch_size):\n",
    "    times = np.array(timings)\n",
    "    steps = len(times)\n",
    "    speeds = batch_size / times\n",
    "    time_mean = np.mean(times)\n",
    "    time_med = np.median(times)\n",
    "    time_99th = np.percentile(times, 99)\n",
    "    time_std = np.std(times, ddof=0)\n",
    "    speed_mean = np.mean(speeds)\n",
    "    speed_med = np.median(speeds)\n",
    "\n",
    "    msg = (\"\\n%s =================================\\n\"\n",
    "            \"batch size=%d, num iterations=%d\\n\"\n",
    "            \"  Median text batches/second: %.1f, mean: %.1f\\n\"\n",
    "            \"  Median latency: %.6f, mean: %.6f, 99th_p: %.6f, std_dev: %.6f\\n\"\n",
    "            ) % (graphName,\n",
    "                batch_size, steps,\n",
    "                speed_med, speed_mean,\n",
    "                time_med, time_mean, time_99th, time_std)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT Torch GPU =================================\n",
      "batch size=1, num iterations=50\n",
      "  Median text batches/second: 159.8, mean: 156.1\n",
      "  Median latency: 0.006259, mean: 0.006544, 99th_p: 0.009934, std_dev: 0.001061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(partial(torch_model.predict, masked_sentences[:1], pos_masks[:1], False))\n",
    "\n",
    "printStats(\"BERT Torch GPU\", timings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT ONNX GPU =================================\n",
      "batch size=1, num iterations=50\n",
      "  Median text batches/second: 92.3, mean: 92.7\n",
      "  Median latency: 0.010836, mean: 0.011059, 99th_p: 0.015159, std_dev: 0.001764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(partial(onnx_model.predict, masked_sentences[:1], pos_masks[:1]))\n",
    "\n",
    "printStats(\"BERT ONNX GPU\", timings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT ONNX TRT GPU =================================\n",
      "batch size=1, num iterations=50\n",
      "  Median text batches/second: 290.4, mean: 277.7\n",
      "  Median latency: 0.003444, mean: 0.003658, 99th_p: 0.005146, std_dev: 0.000506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(partial(trt_model.predict, masked_sentences[:1], pos_masks[:1], False))\n",
    "\n",
    "printStats(\"BERT ONNX TRT GPU\", timings, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "从 GPU 复制到 CPU 很耗时.\n",
    "\n",
    "目前的结果汇总如下\n",
    "\n",
    "| 模型 | 返回数据类型 | 每秒请求次数 |\n",
    "| -- | -- | --|\n",
    "| torch | cuda tensor | 156 |\n",
    "| torch | numpy | 120 |\n",
    "| onnx | numpy | 92 |\n",
    "| onnx_trt | cuda tensor | 277 |\n",
    "| onnx_trt | numpy | 182 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
