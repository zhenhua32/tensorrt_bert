{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考来源\n",
    "\n",
    "- [Inference PyTorch Bert Model with ONNX Runtime on GPU](https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/notebooks/PyTorch_Bert-Squad_OnnxRuntime_GPU.ipynb)\n",
    "- [transformers to onnx](https://huggingface.co/docs/transformers/v4.25.1/en/serialization#export-to-onnx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先必须安装依赖, onnxruntime 的 python 包也是分为 CPU 版和 GPU 版的.\n",
    "\n",
    "- onnxruntime\n",
    "- onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: onnxruntime-gpu in /opt/conda/lib/python3.8/site-packages (1.12.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.8/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.8/site-packages (from onnxruntime-gpu) (1.22.3)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.8/site-packages (from onnxruntime-gpu) (3.20.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from onnxruntime-gpu) (1.11.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from onnxruntime-gpu) (21.3)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.8/site-packages (from onnxruntime-gpu) (2.0.7)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.8/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->onnxruntime-gpu) (3.0.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy->onnxruntime-gpu) (1.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install netron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查 onnxruntime 环境已经安装正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "GPU\n",
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "print(onnxruntime.__version__)\n",
    "print(onnxruntime.get_device())\n",
    "print(onnxruntime.get_available_providers())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的, 这次也是使用 BertForMaskedLM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from transformers import BertTokenizer\n",
    "enc = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "masked_sentences = ['Paris is the [MASK] of France.', \n",
    "                    'The primary [MASK] of the United States is English.', \n",
    "                    'A baseball game consists of at least nine [MASK].', \n",
    "                    'Topology is a branch of [MASK] concerned with the properties of geometric objects that remain unchanged under continuous transformations.']\n",
    "pos_masks = [4, 3, 9, 6]\n",
    "\n",
    "inputs = enc(masked_sentences, return_tensors=\"np\", padding='max_length', max_length=128, truncation=True)\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "origin_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", torchscript=True).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换成 ONNX 模型\n",
    "\n",
    "可以直接使用 transformers.onnx 这个命令行转换模型, 我这里使用了特性头 `--feature=masked-lm`, 因为要和 BertForMaskedLM 类保持一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework not requested. Using torch to export to ONNX.\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using framework PyTorch: 1.12.0a0+8a1a93a\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'logits'})\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (3, 9, 30522) matches (3, 9, 30522)\n",
      "\t\t-[x] values not close enough (atol: 1e-05)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/onnx/__main__.py\", line 180, in <module>\n",
      "    main()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/onnx/__main__.py\", line 173, in main\n",
      "    validate_model_outputs(onnx_config, preprocessor, model, args.output, onnx_outputs, args.atol)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/onnx/convert.py\", line 472, in validate_model_outputs\n",
      "    raise ValueError(\n",
      "ValueError: Outputs values doesn't match between reference model and ONNX exported model: Got max absolute difference of: 0.00017833709716796875 for [ 0.39481845  0.5781618  -0.3123433  ... -2.2203553  -0.26061893\n",
      "  0.15988564] vs [ 0.39483854  0.57817936 -0.3123294  ... -2.2203958  -0.26063988\n",
      "  0.1598672 ]\n"
     ]
    }
   ],
   "source": [
    "# 本地转换模型还是有点报错的, 输出里提到绝对误差超过了 1e-5\n",
    "!python -m transformers.onnx --model=bert-base-uncased --feature=masked-lm onnx/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 torch.onnx.export 转换模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids int64 (4, 128)\n",
      "token_type_ids int64 (4, 128)\n",
      "attention_mask int64 (4, 128)\n"
     ]
    }
   ],
   "source": [
    "for key, val in inputs.items():\n",
    "    print(key, val.dtype, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-7.1718, -7.2419, -7.6383,  ..., -7.2194, -7.9306, -2.7548],\n",
       "          [-7.2182, -7.3509, -7.7468,  ..., -7.3301, -7.9822, -2.9581],\n",
       "          [-7.1483, -7.2817, -7.6816,  ..., -7.2920, -7.9250, -2.8913],\n",
       "          ...,\n",
       "          [-7.1612, -7.3135, -7.6708,  ..., -7.1626, -7.9145, -2.7564],\n",
       "          [-7.1650, -7.3228, -7.6832,  ..., -7.1629, -7.9353, -2.7415],\n",
       "          [-7.2170, -7.3762, -7.7244,  ..., -7.2227, -7.9912, -2.7825]],\n",
       " \n",
       "         [[-7.0209, -7.0549, -7.4369,  ..., -7.1138, -7.6105, -2.9733],\n",
       "          [-7.0949, -7.2220, -7.5740,  ..., -7.2443, -7.7694, -3.1173],\n",
       "          [-7.0031, -7.1278, -7.4964,  ..., -7.1937, -7.7031, -3.0237],\n",
       "          ...,\n",
       "          [-6.9061, -7.0161, -7.3499,  ..., -6.9274, -7.5017, -3.0070],\n",
       "          [-6.9416, -7.0478, -7.3914,  ..., -6.9721, -7.5609, -2.8657],\n",
       "          [-6.9180, -7.0231, -7.3659,  ..., -6.9642, -7.5726, -2.8618]],\n",
       " \n",
       "         [[-7.0116, -7.0444, -7.4240,  ..., -7.1217, -7.5758, -3.0891],\n",
       "          [-7.3173, -7.4623, -7.8499,  ..., -7.4884, -7.9329, -3.2673],\n",
       "          [-7.2589, -7.3994, -7.7982,  ..., -7.4539, -7.8722, -3.1597],\n",
       "          ...,\n",
       "          [-7.1961, -7.3305, -7.6714,  ..., -7.2736, -7.8290, -3.1479],\n",
       "          [-7.2431, -7.4019, -7.7419,  ..., -7.3547, -7.9255, -2.9960],\n",
       "          [-7.2207, -7.3822, -7.7160,  ..., -7.3619, -7.9270, -2.9458]],\n",
       " \n",
       "         [[-7.1075, -7.1435, -7.5437,  ..., -7.2410, -7.7014, -3.1385],\n",
       "          [-7.1835, -7.2990, -7.6779,  ..., -7.4265, -7.8748, -3.1204],\n",
       "          [-7.3067, -7.4359, -7.8321,  ..., -7.5230, -7.9415, -3.1223],\n",
       "          ...,\n",
       "          [-7.2799, -7.4117, -7.7523,  ..., -7.3930, -7.8861, -3.0936],\n",
       "          [-7.3457, -7.4886, -7.8402,  ..., -7.4958, -7.9851, -2.9842],\n",
       "          [-7.3645, -7.5117, -7.8632,  ..., -7.5132, -8.0550, -2.9990]]],\n",
       "        grad_fn=<ViewBackward0>),)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(0, 100, (4, 128), device=\"cpu\", dtype=torch.int64)\n",
    "attention_mask = torch.randint(0, 2, (4, 128), device=\"cpu\", dtype=torch.int64)\n",
    "token_type_ids = torch.randint(0, 2, (4, 128), device=\"cpu\", dtype=torch.int64)\n",
    "\n",
    "traced_origin_model = torch.jit.trace(origin_model, [input_ids, attention_mask, token_type_ids])\n",
    "traced_origin_model(input_ids, attention_mask, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/onnx/utils.py:368: UserWarning: Model has no forward function\n",
      "  warnings.warn(\"Model has no forward function\")\n"
     ]
    }
   ],
   "source": [
    "# 导出 ONNX 模型\n",
    "symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "torch.onnx.export(\n",
    "    traced_origin_model,\n",
    "    args=(input_ids, attention_mask, token_type_ids),\n",
    "    f=\"./onnx/model_torch.onnx\",\n",
    "    export_params=True,\n",
    "    verbose=True,\n",
    "    input_names=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "    output_names=[\"logits\"],\n",
    "    opset_version=14,\n",
    "    do_constant_folding=True,\n",
    "    # 设置动态 shape\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": symbolic_names,\n",
    "        \"attention_mask\" : symbolic_names,\n",
    "        \"token_type_ids\" : symbolic_names,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx.checker.check_model(onnx.load(\"./onnx/model_torch.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torchscript\": true,\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers import optimizer\n",
    "optimized_model = optimizer.optimize_model(\"./onnx/model_torch.onnx\", model_type=\"bert\", num_heads=12, hidden_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_useless_cast_nodes: Removed 0 Cast nodes with output type same as input\n",
      "               apply: Fused LayerNormalization count: 26\n",
      "               apply: Fused Gelu count: 13\n",
      "               apply: Fused SkipLayerNormalization count: 24\n",
      "               apply: Fused Attention count: 12\n",
      "         prune_graph: Graph pruned: 0 inputs, 0 outputs and 125 nodes are removed\n",
      "               apply: Fused EmbedLayerNormalization(with mask) count: 1\n",
      "         prune_graph: Graph pruned: 0 inputs, 0 outputs and 10 nodes are removed\n",
      "               apply: Fused BiasGelu count: 13\n",
      "               apply: Fused SkipLayerNormalization(add bias) count: 24\n",
      "            optimize: opset version: 14\n",
      "  save_model_to_file: Sort graphs in topological order\n",
      "  save_model_to_file: Model saved to ./onnx/model_torch_fp32.onnx\n",
      "get_fused_operator_statistics: Optimized operators:{'EmbedLayerNormalization': 1, 'Attention': 12, 'Gelu': 0, 'FastGelu': 0, 'BiasGelu': 13, 'LayerNormalization': 1, 'SkipLayerNormalization': 24}\n",
      "                main: The model has been optimized.\n"
     ]
    }
   ],
   "source": [
    "# 转换成 fp32 模型\n",
    "!python -m onnxruntime.transformers.optimizer --input \"./onnx/model_torch.onnx\" --output \"./onnx/model_torch_fp32.onnx\" \\\n",
    "    --model_type bert --num_heads 12 --hidden_size 768 --use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving './onnx/model_torch_fp32.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "netron.start(\"./onnx/model_torch_fp32.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_useless_cast_nodes: Removed 0 Cast nodes with output type same as input\n",
      "               apply: Fused SkipLayerNormalization count: 24\n",
      "               apply: Fused Attention count: 12\n",
      "         prune_graph: Graph pruned: 0 inputs, 0 outputs and 125 nodes are removed\n",
      "               apply: Fused EmbedLayerNormalization(with mask) count: 1\n",
      "         prune_graph: Graph pruned: 0 inputs, 0 outputs and 10 nodes are removed\n",
      "               apply: Fused BiasGelu count: 13\n",
      "               apply: Fused SkipLayerNormalization(add bias) count: 24\n",
      "            optimize: opset version: 14\n",
      "  save_model_to_file: Sort graphs in topological order\n",
      "  save_model_to_file: Model saved to ./onnx/model_torch_fp16.onnx\n",
      "get_fused_operator_statistics: Optimized operators:{'EmbedLayerNormalization': 1, 'Attention': 12, 'Gelu': 0, 'FastGelu': 0, 'BiasGelu': 13, 'LayerNormalization': 1, 'SkipLayerNormalization': 24}\n",
      "                main: The model has been optimized.\n"
     ]
    }
   ],
   "source": [
    "# 转换成 fp16 模型\n",
    "!python -m onnxruntime.transformers.optimizer --input \"./onnx/model_torch.onnx\" --output \"./onnx/model_torch_fp16.onnx\" \\\n",
    "    --model_type bert --num_heads 12 --hidden_size 768 --use_gpu --float16 --opt_level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test setting TestSetting(batch_size=1, sequence_length=128, test_cases=100, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=None, seed=3, verbose=False)\n",
      "Generating 100 samples for batch_size=1 sequence_length=128\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=24,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 11.60 ms, Throughput = 86.24 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=15,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 12.28 ms, Throughput = 81.42 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=14,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 12.61 ms, Throughput = 79.32 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=13,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 11.75 ms, Throughput = 85.13 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=12,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 12.35 ms, Throughput = 80.98 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=11,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 12.17 ms, Throughput = 82.15 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=10,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 11.53 ms, Throughput = 86.71 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=9,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 12.34 ms, Throughput = 81.06 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=8,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 12.37 ms, Throughput = 80.81 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=7,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 14.18 ms, Throughput = 70.51 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=6,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 12.46 ms, Throughput = 80.24 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=5,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 12.18 ms, Throughput = 82.09 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=4,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 12.11 ms, Throughput = 82.60 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 12.49 ms, Throughput = 80.09 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=2,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 14.13 ms, Throughput = 70.79 QPS\n",
      "Running test: model=model_torch.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=1,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 15.52 ms, Throughput = 64.44 QPS\n",
      "Test summary is saved to onnx/perf_results_GPU_B1_S128_20221221-143226.txt\n"
     ]
    }
   ],
   "source": [
    "!python -m onnxruntime.transformers.bert_perf_test --model \"./onnx/model_torch.onnx\" \\\n",
    "    --batch_size 1 --sequence_length 128 --samples 100 --test_times 1 --use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test setting TestSetting(batch_size=1, sequence_length=128, test_cases=100, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=None, seed=3, verbose=False)\n",
      "Generating 100 samples for batch_size=1 sequence_length=128\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=24,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.81 ms, Throughput = 92.51 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=15,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 11.05 ms, Throughput = 90.53 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=14,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.82 ms, Throughput = 92.40 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=13,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.41 ms, Throughput = 96.05 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=12,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.50 ms, Throughput = 95.23 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=11,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.31 ms, Throughput = 96.98 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=10,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.87 ms, Throughput = 92.02 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=9,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 11.82 ms, Throughput = 84.63 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=8,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.44 ms, Throughput = 95.75 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=7,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.94 ms, Throughput = 91.38 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=6,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.45 ms, Throughput = 95.73 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=5,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.65 ms, Throughput = 93.85 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=4,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.44 ms, Throughput = 95.82 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.74 ms, Throughput = 93.14 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=2,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.22 ms, Throughput = 97.87 QPS\n",
      "Running test: model=model_torch_fp32.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=1,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.92 ms, Throughput = 91.56 QPS\n",
      "Test summary is saved to onnx/perf_results_GPU_B1_S128_20221221-144509.txt\n"
     ]
    }
   ],
   "source": [
    "!python -m onnxruntime.transformers.bert_perf_test --model \"./onnx/model_torch_fp32.onnx\" \\\n",
    "    --batch_size 1 --sequence_length 128 --samples 100 --test_times 1 --use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test setting TestSetting(batch_size=1, sequence_length=128, test_cases=100, test_times=1, use_gpu=True, use_io_binding=False, provider=None, intra_op_num_threads=None, seed=3, verbose=False)\n",
      "Generating 100 samples for batch_size=1 sequence_length=128\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=24,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.00 ms, Throughput = 100.04 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=15,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.61 ms, Throughput = 94.27 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=14,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.22 ms, Throughput = 97.83 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=13,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.15 ms, Throughput = 98.53 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=12,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.19 ms, Throughput = 98.09 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=11,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.22 ms, Throughput = 97.86 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=10,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 9.97 ms, Throughput = 100.25 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=9,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 11.17 ms, Throughput = 89.49 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=8,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 9.69 ms, Throughput = 103.20 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=7,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 9.78 ms, Throughput = 102.25 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=6,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.32 ms, Throughput = 96.89 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=5,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.09 ms, Throughput = 99.16 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=4,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 9.95 ms, Throughput = 100.52 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=3,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.52 ms, Throughput = 95.08 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=2,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 9.71 ms, Throughput = 102.96 QPS\n",
      "Running test: model=model_torch_fp16.onnx,graph_optimization_level=ENABLE_ALL,intra_op_num_threads=1,batch_size=1,sequence_length=128,test_cases=100,test_times=1,use_gpu=True\n",
      "Average latency = 10.14 ms, Throughput = 98.61 QPS\n",
      "Test summary is saved to onnx/perf_results_GPU_B1_S128_20221221-144612.txt\n"
     ]
    }
   ],
   "source": [
    "# 为啥我这边用 fp16 就没啥效果, 我看原始的notebook里有三倍提升\n",
    "!python -m onnxruntime.transformers.bert_perf_test --model \"./onnx/model_torch_fp16.onnx\" \\\n",
    "    --batch_size 1 --sequence_length 128 --samples 100 --test_times 1 --use_gpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载 ONNX 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "\n",
    "# 加载 ONNX 模型\n",
    "session = InferenceSession(\"onnx/model_torch_fp16.onnx\", providers=[\"CUDAExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入:\n",
      "['input_ids', 'attention_mask', 'token_type_ids']\n",
      "[['batch_size', 'max_seq_len'], ['batch_size', 'max_seq_len'], ['batch_size', 'max_seq_len']]\n",
      "['tensor(int32)', 'tensor(int32)', 'tensor(int32)']\n",
      "输出:\n",
      "['logits']\n",
      "[['batch_size', 'max_seq_len', 30522]]\n",
      "['tensor(float)']\n"
     ]
    }
   ],
   "source": [
    "print(\"输入:\")\n",
    "print([x.name for x in session.get_inputs()])\n",
    "print([x.shape for x in session.get_inputs()])\n",
    "print([x.type for x in session.get_inputs()])\n",
    "\n",
    "print(\"输出:\")\n",
    "print([x.name for x in session.get_outputs()])\n",
    "print([x.shape for x in session.get_outputs()])\n",
    "print([x.type for x in session.get_outputs()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(int64)) , expected: (tensor(int32))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/workspace/examples/torch_tensorrt/notebooks/new_onnx.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7570626561745f72696465222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/workspace/examples/torch_tensorrt/notebooks/new_onnx.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 进行推理, 推理时注意, 模型的输入是 numpy array 类型\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7570626561745f72696465222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/workspace/examples/torch_tensorrt/notebooks/new_onnx.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m outputs \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrun(output_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m\"\u001b[39;49m], input_feed\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(inputs))\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7570626561745f72696465222c22637764223a225c5c5c5c77736c2e6c6f63616c686f73745c5c5562756e74752d32322e30345c5c227d/workspace/examples/torch_tensorrt/notebooks/new_onnx.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m outputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:200\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    198\u001b[0m     output_names \u001b[39m=\u001b[39m [output\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_meta]\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sess\u001b[39m.\u001b[39;49mrun(output_names, input_feed, run_options)\n\u001b[1;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m C\u001b[39m.\u001b[39mEPFail \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(int64)) , expected: (tensor(int32))"
     ]
    }
   ],
   "source": [
    "# 进行推理, 推理时注意, 模型的输入是 numpy array 类型\n",
    "outputs = session.run(output_names=[\"logits\"], input_feed=dict(inputs))\n",
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3007, 2653, 7202, 5597]\n",
      "Paris is the capital of France.\n",
      "The primary language of the United States is English.\n",
      "A baseball game consists of at least nine innings.\n",
      "Topology is a branch of mathematics concerned with the properties of geometric objects that remain unchanged under continuous transformations.\n"
     ]
    }
   ],
   "source": [
    "most_likely_token_ids = [numpy.argmax(outputs[0][i, pos, :]) for i, pos in enumerate(pos_masks)]\n",
    "print(most_likely_token_ids)\n",
    "unmasked_tokens = enc.decode(most_likely_token_ids).split(' ')\n",
    "unmasked_sentences = [masked_sentences[i].replace('[MASK]', token) for i, token in enumerate(unmasked_tokens)]\n",
    "for sentence in unmasked_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(3007), tensor(2653), tensor(7202), tensor(5597)]\n",
      "Paris is the capital of France.\n",
      "The primary language of the United States is English.\n",
      "A baseball game consists of at least nine innings.\n",
      "Topology is a branch of mathematics concerned with the properties of geometric objects that remain unchanged under continuous transformations.\n"
     ]
    }
   ],
   "source": [
    "# 和原始模型对照下\n",
    "inputs_pt = enc(masked_sentences, return_tensors=\"pt\", padding='max_length', max_length=128, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs_origin = origin_model(**inputs_pt)\n",
    "\n",
    "most_likely_token_ids = [torch.argmax(outputs_origin[0][i, pos, :]) for i, pos in enumerate(pos_masks)]\n",
    "print(most_likely_token_ids)\n",
    "unmasked_tokens = enc.decode(most_likely_token_ids).split(' ')\n",
    "unmasked_sentences = [masked_sentences[i].replace('[MASK]', token) for i, token in enumerate(unmasked_tokens)]\n",
    "for sentence in unmasked_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个有毒, 差距有点大, 1e-3 都满足不了\n",
    "a = outputs[0]\n",
    "b = outputs_origin[0].detach().numpy()\n",
    "numpy.allclose(a, b, rtol=1e-03, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.5416336, -6.5075865, -6.5212126, ..., -5.8960814, -5.7351847,\n",
       "       -3.8943403], dtype=float32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.5419493, -6.50757  , -6.521825 , ..., -5.8963876, -5.7355714,\n",
       "       -3.8943367], dtype=float32)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 0, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试性能\n",
    "\n",
    "注意: 不同序列长度对模型的影响很大, 目前来看 ONNX 比较适合短序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 准备下模型, 以及调用函数\n",
    "session_cpu = InferenceSession(\"onnx/model_torch_fp32.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "session_gpu = InferenceSession(\"onnx/model_torch_fp32.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "origin_model_cpu = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").eval()\n",
    "origin_model_gpu = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").cuda().eval()\n",
    "\n",
    "# 应该要让模型的输出是一致的, 都返回 logits, numpy 格式的\n",
    "def call_onnx_cpu():\n",
    "    inputs = enc(masked_sentences, return_tensors=\"np\", padding='max_length', max_length=128, truncation=True)\n",
    "    return session_cpu.run(output_names=[\"logits\"], input_feed=dict(inputs))[0]\n",
    "\n",
    "\n",
    "def call_onnx_gpu():\n",
    "    inputs = enc(masked_sentences, return_tensors=\"np\", padding='max_length', max_length=128, truncation=True)\n",
    "    return session_gpu.run(output_names=[\"logits\"], input_feed=dict(inputs))[0]\n",
    "\n",
    "def call_torch_cpu():\n",
    "    inputs_pt = enc(masked_sentences, return_tensors=\"pt\", padding='max_length', max_length=128, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        return origin_model_cpu(**inputs_pt)[0].numpy()\n",
    "\n",
    "def call_torch_gpu():\n",
    "    inputs_pt = enc(masked_sentences, return_tensors=\"pt\", padding='max_length', max_length=128, truncation=True)\n",
    "    inputs_pt_gpu = dict({k: v.cuda() for k, v in inputs_pt.items()})\n",
    "    with torch.no_grad():\n",
    "        return origin_model_gpu(**inputs_pt_gpu)[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "def timeGraph(call_func, num_loops=50):\n",
    "    print(\"Warm up ...\")\n",
    "    for _ in range(20):\n",
    "        call_func()\n",
    "\n",
    "    # 等待同步, cuda 默认是异步调用的\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    for i in range(num_loops):\n",
    "        start_time = timeit.default_timer()\n",
    "        call_func()\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = timeit.default_timer()\n",
    "        timings.append(end_time - start_time)\n",
    "        # print(\"Iteration {}: {:.6f} s\".format(i, end_time - start_time))\n",
    "\n",
    "    return timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def printStats(graphName, timings, batch_size):\n",
    "    times = np.array(timings)\n",
    "    steps = len(times)\n",
    "    speeds = batch_size / times\n",
    "    time_mean = np.mean(times)\n",
    "    time_med = np.median(times)\n",
    "    time_99th = np.percentile(times, 99)\n",
    "    time_std = np.std(times, ddof=0)\n",
    "    speed_mean = np.mean(speeds)\n",
    "    speed_med = np.median(speeds)\n",
    "\n",
    "    msg = (\"\\n%s =================================\\n\"\n",
    "            \"batch size=%d, num iterations=%d\\n\"\n",
    "            \"  Median text batches/second: %.1f, mean: %.1f\\n\"\n",
    "            \"  Median latency: %.6f, mean: %.6f, 99th_p: %.6f, std_dev: %.6f\\n\"\n",
    "            ) % (graphName,\n",
    "                batch_size, steps,\n",
    "                speed_med, speed_mean,\n",
    "                time_med, time_mean, time_99th, time_std)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT ONNX CPU =================================\n",
      "batch size=1, num iterations=50\n",
      "  Median text batches/second: 7.3, mean: 7.3\n",
      "  Median latency: 0.136545, mean: 0.136472, 99th_p: 0.150455, std_dev: 0.006062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(call_onnx_cpu)\n",
    "\n",
    "printStats(\"BERT ONNX CPU\", timings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT TORCH CPU =================================\n",
      "batch size=1, num iterations=50\n",
      "  Median text batches/second: 28.9, mean: 28.8\n",
      "  Median latency: 0.034642, mean: 0.034878, 99th_p: 0.040572, std_dev: 0.002582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(call_torch_cpu)\n",
    "\n",
    "printStats(\"BERT TORCH CPU\", timings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT ONNX GPU =================================\n",
      "batch size=1, num iterations=100\n",
      "  Median text batches/second: 40.8, mean: 41.0\n",
      "  Median latency: 0.024493, mean: 0.024562, 99th_p: 0.029658, std_dev: 0.001960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(call_onnx_gpu, 100)\n",
    "\n",
    "printStats(\"BERT ONNX GPU\", timings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT TORCH GPU =================================\n",
      "batch size=1, num iterations=100\n",
      "  Median text batches/second: 51.8, mean: 51.8\n",
      "  Median latency: 0.019303, mean: 0.019462, 99th_p: 0.024125, std_dev: 0.001726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timings = timeGraph(call_torch_gpu, 100)\n",
    "\n",
    "printStats(\"BERT TORCH GPU\", timings, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: ONNX 的 CPU 比 Torch 的 CPU 快一点, 但 GPU 慢很多, 不知道是什么情况?\n",
    "现在看来短序列在 GPU 上是有优势的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -6.5419493,  -6.50757  ,  -6.521825 , ...,  -5.8963876,\n",
       "          -5.7355714,  -3.8943367],\n",
       "        [ -9.013971 ,  -9.046798 ,  -9.060098 , ...,  -8.257676 ,\n",
       "          -8.033421 ,  -6.1780944],\n",
       "        [ -8.652058 ,  -9.085825 ,  -8.771672 , ...,  -7.4472957,\n",
       "          -5.338148 ,  -9.655459 ],\n",
       "        ...,\n",
       "        [ -8.809074 ,  -9.010075 ,  -8.932358 , ...,  -8.146495 ,\n",
       "          -9.273098 ,  -5.312276 ],\n",
       "        [ -8.758413 ,  -8.874108 ,  -8.888932 , ...,  -8.339445 ,\n",
       "          -9.370628 ,  -4.932161 ],\n",
       "        [ -8.783895 ,  -9.026108 ,  -8.919375 , ...,  -8.316187 ,\n",
       "          -8.880913 ,  -6.533851 ]],\n",
       "\n",
       "       [[ -6.6346483,  -6.600541 ,  -6.5921936, ...,  -5.9096456,\n",
       "          -5.8169036,  -4.149713 ],\n",
       "        [-12.518587 , -12.860065 , -12.95737  , ..., -13.159752 ,\n",
       "         -10.315203 , -14.825867 ],\n",
       "        [ -7.4197383,  -7.990819 ,  -7.946541 , ...,  -8.570905 ,\n",
       "          -5.710056 , -11.684243 ],\n",
       "        ...,\n",
       "        [ -6.4807057,  -6.4920845,  -6.5437775, ...,  -6.9808264,\n",
       "          -7.774022 ,  -3.5951703],\n",
       "        [ -7.1094484,  -7.2465587,  -7.2549877, ...,  -7.85568  ,\n",
       "          -7.5810375,  -6.5985684],\n",
       "        [ -7.0686884,  -7.287115 ,  -7.277562 , ...,  -7.855511 ,\n",
       "          -7.3587685,  -7.509465 ]],\n",
       "\n",
       "       [[ -6.6421857,  -6.6100698,  -6.600312 , ...,  -5.9668145,\n",
       "          -5.687108 ,  -3.8447714],\n",
       "        [-12.993292 , -12.744807 , -13.165543 , ..., -12.114578 ,\n",
       "          -9.17845  ,  -8.154595 ],\n",
       "        [ -8.056169 ,  -8.577609 ,  -7.9242954, ...,  -6.383187 ,\n",
       "          -9.1368065,  -4.0645137],\n",
       "        ...,\n",
       "        [ -7.203394 ,  -7.0101047,  -7.3212595, ...,  -6.3296685,\n",
       "          -7.562246 ,  -3.8699157],\n",
       "        [ -7.604661 ,  -7.4904428,  -7.6505423, ...,  -6.610661 ,\n",
       "          -7.4156923,  -5.7997875],\n",
       "        [ -7.4720216,  -7.3855214,  -7.5408077, ...,  -6.5134234,\n",
       "          -7.4362645,  -5.3732586]],\n",
       "\n",
       "       [[ -7.0696654,  -7.0060225,  -6.993084 , ...,  -6.183702 ,\n",
       "          -6.079462 ,  -4.7723413],\n",
       "        [ -4.4957447,  -4.3197436,  -4.0417457, ...,  -4.0363426,\n",
       "          -4.1039953,  -3.3383453],\n",
       "        [ -9.554264 ,  -9.49653  ,  -9.600105 , ...,  -9.135786 ,\n",
       "          -6.61602  ,  -9.301937 ],\n",
       "        ...,\n",
       "        [ -6.195328 ,  -5.8787646,  -5.805171 , ...,  -6.060064 ,\n",
       "          -5.6603136,  -5.478305 ],\n",
       "        [ -3.794194 ,  -3.754062 ,  -3.4975975, ...,  -4.507863 ,\n",
       "          -4.466552 ,  -4.749577 ],\n",
       "        [ -6.6573205,  -6.437923 ,  -6.3678904, ...,  -7.000005 ,\n",
       "          -5.972591 ,  -5.548209 ]]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = enc(masked_sentences, return_tensors=\"np\", padding='max_length', max_length=128)\n",
    "\n",
    "io_binding = session_gpu.io_binding()\n",
    "for key, val in inputs.items():\n",
    "    io_binding.bind_cpu_input(key, val)\n",
    "io_binding.bind_output('logits')\n",
    "session_gpu.run_with_iobinding(io_binding)\n",
    "logits = io_binding.copy_outputs_to_cpu()[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT ONNX GPU =================================\n",
      "batch size=1, num iterations=50\n",
      "  Median text batches/second: 35.3, mean: 35.2\n",
      "  Median latency: 0.028292, mean: 0.028495, 99th_p: 0.031135, std_dev: 0.001489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def call_test():\n",
    "    inputs = enc(masked_sentences, return_tensors=\"np\", padding='max_length', max_length=128)\n",
    "\n",
    "    io_binding = session_gpu.io_binding()\n",
    "    for key, val in inputs.items():\n",
    "        io_binding.bind_cpu_input(key, val)\n",
    "    io_binding.bind_output('logits')\n",
    "    session_gpu.run_with_iobinding(io_binding)\n",
    "    logits = io_binding.copy_outputs_to_cpu()[0]\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "timings = timeGraph(call_test)\n",
    "\n",
    "printStats(\"BERT ONNX GPU\", timings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-4.934188  , -5.0961747 , -5.1510315 , ..., -4.956326  ,\n",
       "         -6.8429976 ,  0.82923543],\n",
       "        [-4.7176423 , -4.9170785 , -4.97676   , ..., -4.660313  ,\n",
       "         -6.5653915 ,  0.71815693],\n",
       "        [-4.355648  , -4.5349092 , -4.6175747 , ..., -4.3954697 ,\n",
       "         -6.210213  ,  0.9891236 ],\n",
       "        ...,\n",
       "        [-4.6253095 , -4.8404446 , -4.8514004 , ..., -4.715632  ,\n",
       "         -6.5525537 ,  0.5545111 ],\n",
       "        [-4.5998197 , -4.8183794 , -4.844186  , ..., -4.71098   ,\n",
       "         -6.552678  ,  0.6832975 ],\n",
       "        [-4.7034082 , -4.911319  , -4.9586306 , ..., -4.7842    ,\n",
       "         -6.604789  ,  0.66530704]],\n",
       "\n",
       "       [[-4.7924905 , -4.9325333 , -5.0450907 , ..., -4.6667776 ,\n",
       "         -6.526173  ,  0.33888003],\n",
       "        [-4.5583034 , -4.7254114 , -4.8429117 , ..., -4.3630447 ,\n",
       "         -6.245224  ,  0.24502823],\n",
       "        [-4.17154   , -4.3237524 , -4.46185   , ..., -4.1069174 ,\n",
       "         -5.8931756 ,  0.5509099 ],\n",
       "        ...,\n",
       "        [-4.4804244 , -4.6598744 , -4.725569  , ..., -4.411356  ,\n",
       "         -6.152767  ,  0.10763165],\n",
       "        [-4.4503155 , -4.6309247 , -4.707715  , ..., -4.428321  ,\n",
       "         -6.182623  ,  0.1874496 ],\n",
       "        [-4.569488  , -4.745061  , -4.843052  , ..., -4.5447845 ,\n",
       "         -6.302166  ,  0.11770371]],\n",
       "\n",
       "       [[-4.7924905 , -4.9325333 , -5.0450907 , ..., -4.6667776 ,\n",
       "         -6.526173  ,  0.33888003],\n",
       "        [-4.5583034 , -4.7254114 , -4.8429117 , ..., -4.3630447 ,\n",
       "         -6.245224  ,  0.24502823],\n",
       "        [-4.17154   , -4.3237524 , -4.46185   , ..., -4.1069174 ,\n",
       "         -5.8931756 ,  0.5509099 ],\n",
       "        ...,\n",
       "        [-4.4804244 , -4.6598744 , -4.725569  , ..., -4.411356  ,\n",
       "         -6.152767  ,  0.10763165],\n",
       "        [-4.4503155 , -4.6309247 , -4.707715  , ..., -4.428321  ,\n",
       "         -6.182623  ,  0.1874496 ],\n",
       "        [-4.569488  , -4.745061  , -4.843052  , ..., -4.5447845 ,\n",
       "         -6.302166  ,  0.11770371]],\n",
       "\n",
       "       [[-4.643123  , -4.6886005 , -4.9222136 , ..., -4.442108  ,\n",
       "         -6.301814  , -0.8561482 ],\n",
       "        [-4.4936285 , -4.5541377 , -4.7881145 , ..., -4.207207  ,\n",
       "         -6.1254215 , -1.0190252 ],\n",
       "        [-4.121963  , -4.179811  , -4.4148393 , ..., -3.9524016 ,\n",
       "         -5.783214  , -0.7241839 ],\n",
       "        ...,\n",
       "        [-4.533946  , -4.622769  , -4.8384466 , ..., -4.3745832 ,\n",
       "         -6.129344  , -1.1902157 ],\n",
       "        [-4.490959  , -4.5755177 , -4.7963963 , ..., -4.3521256 ,\n",
       "         -6.1154375 , -1.101346  ],\n",
       "        [-4.559624  , -4.639782  , -4.868086  , ..., -4.40383   ,\n",
       "         -6.1721625 , -1.1532053 ]]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: 这个输出不对\n",
    "io_binding = session_gpu.io_binding()\n",
    "for key, val in inputs.items():\n",
    "    X_ortvalue = onnxruntime.OrtValue.ortvalue_from_numpy(val, 'cuda', 0)\n",
    "    io_binding.bind_input(name=key, device_type=X_ortvalue.device_name(), device_id=0, element_type=val.dtype, shape=X_ortvalue.shape(), buffer_ptr=X_ortvalue.data_ptr())\n",
    "io_binding.bind_output(\"logits\")\n",
    "session_gpu.run_with_iobinding(io_binding)\n",
    "logits = io_binding.copy_outputs_to_cpu()[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "\n",
      "BERT ONNX GPU =================================\n",
      "batch size=1, num iterations=50\n",
      "  Median text batches/second: 65.8, mean: 65.3\n",
      "  Median latency: 0.015200, mean: 0.015383, 99th_p: 0.018203, std_dev: 0.001020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def call_test():\n",
    "    io_binding = session_gpu.io_binding()\n",
    "    for key, val in inputs.items():\n",
    "        X_ortvalue = onnxruntime.OrtValue.ortvalue_from_numpy(val, 'cuda', 0)\n",
    "        io_binding.bind_input(name=key, device_type=X_ortvalue.device_name(), device_id=0, element_type=val.dtype, shape=X_ortvalue.shape(), buffer_ptr=X_ortvalue.data_ptr())\n",
    "    io_binding.bind_output(\"logits\")\n",
    "    session_gpu.run_with_iobinding(io_binding)\n",
    "    logits = io_binding.copy_outputs_to_cpu()[0]\n",
    "    return logits\n",
    "\n",
    "timings = timeGraph(call_test)\n",
    "\n",
    "printStats(\"BERT ONNX GPU\", timings, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
